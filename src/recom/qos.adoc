[#sec:qos]
## Configurable Quality of Service (QoS) and Priority Management

[#sec:qos:enforcement]
### QoS enforcement

RCIDs provide a sufficiently powerful abstraction allowing to define any set of
constraints in any shared component that may be needed for safety reasons.
RCIDs provide an abstraction allowing to set constraints for diverse components,
including interconnects, cache memories, queues, and any other.
Yet, defining constraints must be done with special care since nothing prevents
using RCIDs with incompatible or potentially problematic constraints across
tasks running in different harts.
For instance, it is possible to run tasks whose aggregated bandwidth allocated
in an interconnect is above 100%, which would be incompatible in practice, or
with potentially problematic cache allocations (e.g. task A uses ways 1 and 2,
and task B ways 2 and 3) that provide neither partitioning, nor full sharing.
Also, specific combinations of RCIDs, if used by different concurrent tasks,
could lead to issues such as priority inversion if not defined and used with
care.

Based on their definition, RCIDs could allow expressing virtually any set of
constraints, such as end-to-end constraints (e.g. end-to-end memory latencies),
but how to map RCIDs to specific QoS constraints is completely implementation
dependent.
Therefore, from an ISA perspective, no further ISA support is needed to realize
end-to-end constraints.

One could use RCIDs to express multiple constraints even for a single shared
resource, such as for instance, the virtual channel to use and the bandwidth
allocated within that virtual channel for a NoC, as well as the allocated cache
space and the number of entries allocated in multiple queues in such a cache (to
hold miss requests, eviction requests, etc.).
Since RCIDs can be changed dynamically, even if associated to harts, one could
keep an RCID per task and update the RCID of the hart upon a context switch.
Hence, the scope at which to use RCIDs is completely software dependent and
virtually any required scope can be realized with RCIDs.

RCID management can likely be implemented in the operating system or the
hypervisor.
One could, for instance, link RCIDs to scheduling priorities to provide a simple
user interface.

It remains to be defined how those RCIDs are effectively implemented at
microarchitectural level, but such a definition is beyond RISC-V ISA
specifications.
Hence, while tagging requests with RCIDs and propagating those RCIDs across
cascade requests in other components could be an appropriate implementation,
whether this or another implementation is used is beyond the scope of this
document.

[#sec:qos:monitors]
### QoS-relevant monitors

MCIDs offer a single monitor per component which, for safety purposes, may fall
short since QoS choices may be performed based on multiple monitors.
For instance, one may decide to increase or decrease the service for a task in a
shared L2 cache based on how often such a task accesses the cache, whether it
performs read or write requests, experiences hits or misses, keeps occupancy of
specific queues high or low, etc.
The fact that multiple such metrics would have to be covered by a single monitor
can be regarded as a limitation and some form of safety extension may be needed.

As explained before, virtual components can be defined as a way to define as
many MCIDs as required per physical or logical component.
While this trick would be practically doable, it can be regarded as an
inappropriate use of MCIDs.
Hence, this further encourages the definition of appropriate safety extensions
for safety-related monitoring in general, and safety-related QoS monitoring in
particular.

Safety extensions for monitoring could consist of having an arbitrarily large
(or large enough) set of memory mapped monitors so that a given task can access
as much information as needed.
These safety extensions could be easily combined with the current MCID
definition so that the MCID is used to choose the appropriate set of monitors
to read.
Different tasks with different MCIDs may want to read the same monitor, which
may be mapped into multiple memory locations (e.g. overall interconnect
utilization), or different per-task monitors (e.g. individual interconnect
utilization).

[#sec:qos:propagation]
### QoS IDs propagation

Finally, a concern spanning across both RCIDs and MCIDs is RCID/MCID
propagation.
A number of microarchitectural events such as cache dirty evictions, cascade
requests of the coherence protocol, and I/O generated activity are hard to
attribute to specific tasks.
For instance, in the case of a dirty line eviction from cache, one could
attribute such request to the task evicting the line or to the one modifying
originally the line.
RCIDs and MCIDs are agnostic to those choices, which are fully implementation
dependent (e.g. one may use a specific RCID/MCID for I/O generated activity),
but it is important to make a sound use of RCIDs and MCIDs for those types of
requests also because they may have non-negligible performance effects (e.g.
dirty cache line evictions may occur frequently and saturate memory access).
